---
layout: post
title: Analyzer
subtitle: with Nori
tags: [elasticsearch, analyzer, Nori]
comments: true
---

## NORI 1

Nori 형태소 분석기의 특징

1. 일본어 형태소 분석기 Kuromoji의 엔진을 이용해서 제작
2. Mecab-ko 한국어 사전을 내장하고 있지만 기존 200Mb에서 24Mb로 줄여서 내장시켰다.
3. 인덱싱작업이 엄청 빠르다.
4. 여러 추가 옵션을 간단히 추가 할 수 있다.

Elasticsearch 에는 Nori라는 공식 형태소 분석기가 있다.
설치는 아래와 같이 간단하다. 네트워크통신이 안되는 경우에도 플러그인을 직접 받아 설치 할 수 있다.

```
./elasticsearch-plugin install analysis-nori
```

### Tokenizer 필터

https://www.elastic.co/guide/en/elasticsearch/reference/7.6/analysis-tokenizers.html 에서 필터 확인이 가능

Nori 를 설치하고 아무 설정없이도 아래와 같이 사용 가능하다.

```
GET _analyze
{
  "analyzer": "nori",
  "text": ["경민이와 사슴이"]
}

result 
{
  "tokens" : [
    {
      "token" : "경민",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "사슴",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "word",
      "position" : 3
    }
  ]
}

"경민" 과 "사슴" 즉 명사만 출력되었다.
explan 설정은 default로 false 인데 이를 true 로 바꿔주면 더 자세한 정보를 볼 수 있다.

GET _analyze
{
  "analyzer": "nori",
  "text": ["경민이와 사슴이"],
  "explain": true
 }
 
 result
 
 {
  "detail" : {
    "custom_analyzer" : false,
    "analyzer" : {
      "name" : "org.apache.lucene.analysis.ko.KoreanAnalyzer",
      "tokens" : [
        {
          "token" : "경민",
          "start_offset" : 0,
          "end_offset" : 2,
          "type" : "word",
          "position" : 0,
          "bytes" : "[ea b2 bd eb af bc]",
          "leftPOS" : "NNP(Proper Noun)",
          "morphemes" : null,
          "posType" : "MORPHEME",
          "positionLength" : 1,
          "reading" : null,
          "rightPOS" : "NNP(Proper Noun)",
          "termFrequency" : 1
        },
        {
          "token" : "사슴",
          "start_offset" : 5,
          "end_offset" : 7,
          "type" : "word",
          "position" : 3,
          "bytes" : "[ec 82 ac ec 8a b4]",
          "leftPOS" : "NNG(General Noun)",
          "morphemes" : null,
          "posType" : "MORPHEME",
          "positionLength" : 1,
          "reading" : null,
          "rightPOS" : "NNG(General Noun)",
          "termFrequency" : 1
        }
      ]
    }
  }
}
```

편하고 빠르지만 노리에는 아직 문제점이 많이 존재한다.

엘라스틱서치 커뮤니티에서 회의를 하며 아래와 같은 문제점이 다수 발생하는 것을 서로 공유했다.

```
마케터 라는 단어
GET _analyze
{
  "analyzer": "nori", 
  "text": ["마케터"],
  "explain": true
}
result
{
  "detail" : {
    "custom_analyzer" : false,
    "analyzer" : {
      "name" : "org.apache.lucene.analysis.ko.KoreanAnalyzer",
      "tokens" : [
        {
          "token" : "마",
          "start_offset" : 0,
          "end_offset" : 1,
          "type" : "word",
          "position" : 0,
          "bytes" : "[eb a7 88]",
          "leftPOS" : "NNG(General Noun)",
          "morphemes" : null,
          "posType" : "MORPHEME",
          "positionLength" : 1,
          "reading" : null,
          "rightPOS" : "NNG(General Noun)",
          "termFrequency" : 1
        },
        {
          "token" : "트",
          "start_offset" : 2,
          "end_offset" : 3,
          "type" : "word",
          "position" : 3,
          "bytes" : "[ed 8a b8]",
          "leftPOS" : "VV(Verb)",
          "morphemes" : null,
          "posType" : "MORPHEME",
          "positionLength" : 1,
          "reading" : null,
          "rightPOS" : "VV(Verb)",
          "termFrequency" : 1
        }
      ]
    }
  }
}
결과를 보면 마케터 라는 명사가 나오는것이 아니라 마 라는 명사와 트(다)라는 동사로 나뉘어 지는걸 볼 수 있다.

또 다른 오류를 보자.

GET _analyze
{
  "analyzer": "nori", 
  "text": ["12시에우리는점심을먹는다."],
  "explain": true
}

{
  "detail" : {
    "custom_analyzer" : false,
    "analyzer" : {
      "name" : "org.apache.lucene.analysis.ko.KoreanAnalyzer",
      "tokens" : [
        {
          "token" : "12시에우리는점심을먹는다",
          "start_offset" : 0,
          "end_offset" : 13,
          "type" : "word",
          "position" : 0,
          "bytes" : "[31 32 ec 8b 9c ec 97 90 ec 9a b0 eb a6 ac eb 8a 94 ec a0 90 ec 8b ac ec 9d 84 eb a8 b9 eb 8a 94 eb 8b a4]",
          "leftPOS" : "UNKNOWN(Unknown)",
          "morphemes" : null,
          "posType" : "MORPHEME",
          "positionLength" : 1,
          "reading" : null,
          "rightPOS" : "UNKNOWN(Unknown)",
          "termFrequency" : 1
        }
      ]
    }
  }
}
위와 같이 형태소분석이 전혀 되지 않은 상태가 나온다. 그 이유는 맨 앞에 붙은 숫자 때문인데 숫자를 한글로 대체 하면 만족스런(?) 결과가 나온다. 
하지만 이 결과도 별로 만족스럽진 못하다.

GET _analyze
{
  "analyzer": "nori", 
  "text": ["열두시에우리는점심을먹는다."],
  "explain": false
}

result 
{
  "tokens" : [
    {
      "token" : "열",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "시",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "우리",
      "start_offset" : 4,
      "end_offset" : 6,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "점심",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "먹",
      "start_offset" : 10,
      "end_offset" : 11,
      "type" : "word",
      "position" : 8
    }
  ]
}

열두시의 두 가 사라져버린 결과...
```
이를 해결하기 위해 다음 포스팅은 커스텀 아날라이저를 만드는 방법을 포스팅 하겠다.
